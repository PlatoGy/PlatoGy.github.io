---
layout: post
title: æ€§èƒ½ä¼˜åŒ–PPT2
author: Patrick Gao
date: 2025-03-28 03:23
categories:
  - MISCADA
  - GPU Course
tags:
  - GPU Programming
mermaid: true
math: true
pin: false
---
# ğŸ“š Session 2: Memory Hierarchy

**Lecturer**: Anne Reinarz

---

## ğŸ§ª Exercise 1: Sum Reduction Benchmark

- **SIMD**: 4 plateaus
- **Scalar**: 3 plateaus
- **Performance variability** caused by **CPU Boosting**

### â“ Why SIMD doesn't reach peak performance?

- **Not** due to instruction throughput
- **Memory bandwidth** decreases with vector size

---

## ğŸ§  Memory Hierarchy Overview

### Types of Memory:

1. **Small and fast**
2. **Large and slow**

> Physics prevents memory from being both large and fast.

**Optimization strategy**:

- Restructure algorithms to **keep data in fast memory**

ğŸ”— [Latency reference by Colin Scott](https://colin-scott.github.io/personal_website/research/interactive_latency.html)

---

## ğŸ—‚ï¸ Cache Memory

### Features:

- Hierarchy of small, fast memory
- Stores frequently accessed data

### Issues:

- Frequently accessed data is **unknown beforehand**
- Use **principle of locality** to predict

---

## ğŸ“ Principle of Locality

### ğŸ•’ Temporal Locality

- If data is accessed now, it will likely be accessed again **soon**

### ğŸ—ºï¸ Spatial Locality

- If one address is accessed, nearby addresses will likely be accessed

---

## ğŸ“¥ Cache Access Behavior

- **First access**:
  - Load from main memory to register
  - Store in cache
- **Subsequent accesses**:
  - Load from cache (faster)

---

## ğŸ§® Example: Sum Reduction

```c
float s[16] = 0;
for (i = 0; i < N; i++) {
  s[i%16] += a[i];
}
```

- `s`: good **temporal locality**
- `a`: good **spatial locality**

---

## ğŸ§° Cache Design Questions

1. Where to put loaded data?
2. How to check if data is in cache?
3. What to do when cache is full?

- Address broken into:
  - **Tag**
  - **Index**
  - **Offset**

---

## ğŸ“Œ Direct Mapped Cache

- Each address maps to **one specific cache slot**
- Conflict resolution: **newer replaces older** (LRU)

---

## ğŸ“ Cache Line Size

- Data is loaded one cache line at a time
- 64 bytes is typical
- Optimize algorithms to use **cache-line-sized chunks**

---

## ğŸ”„ Cache Thrashing Example

```c
int a[64], b[64], r = 0;
for (int i = 0; i < 100; i++)
  for (int j = 0; j < 64; j++)
    r += a[j] + b[j];
```

- Although cache is big enough, **a[j] and b[j] map to the same cache line**, causing conflict.

---

## ğŸ§© Cache Associativity Types

| Type              | Description                             |
| ----------------- | --------------------------------------- |
| Direct Mapped     | One memory block maps to one cache line |
| Fully Associative | Can go anywhere, costly to implement    |
| Set Associative   | N-way set (common values: 2, 4, 8, 16)  |

---

## ğŸ“Š Exercise 2 & 3: Bandwidth vs Array Size

### Bandwidth from different levels:

- **L1** (<32KB): ~370GB/s
- **L2** (<512KB): ~100GB/s
- **L3** (<16MB): ~78GB/s
- **RAM**: ~17.5GB/s

> Performance decreases as data moves away from CPU.

---

## ğŸ§  Summary: Hardware Architecture Insights

### Key Factors:

- Instruction count
- Execution efficiency
- Data movement time

### Hardware Parallelism:

- **Sockets**: 1â€“4 CPUs
- **Cores**: 4â€“32 per CPU
- **Vectorization**: 2â€“16 floats per vector
- **Superscalar**: 2â€“8 instructions per cycle