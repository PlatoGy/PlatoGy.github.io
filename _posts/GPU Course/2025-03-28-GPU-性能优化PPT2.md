---
layout: post
title: 性能优化PPT2
author: Patrick Gao
date: 2025-03-28 03:23
categories:
  - MISCADA
  - GPU Course
tags:
  - GPU Programming
mermaid: true
math: true
pin: false
---
# 📚 Session 2: Memory Hierarchy

**Lecturer**: Anne Reinarz

---

## 🧪 Exercise 1: Sum Reduction Benchmark

- **SIMD**: 4 plateaus
- **Scalar**: 3 plateaus
- **Performance variability** caused by **CPU Boosting**

### ❓ Why SIMD doesn't reach peak performance?

- **Not** due to instruction throughput
- **Memory bandwidth** decreases with vector size

---

## 🧠 Memory Hierarchy Overview

### Types of Memory:

1. **Small and fast**
2. **Large and slow**

> Physics prevents memory from being both large and fast.

**Optimization strategy**:

- Restructure algorithms to **keep data in fast memory**

🔗 [Latency reference by Colin Scott](https://colin-scott.github.io/personal_website/research/interactive_latency.html)

---

## 🗂️ Cache Memory

### Features:

- Hierarchy of small, fast memory
- Stores frequently accessed data

### Issues:

- Frequently accessed data is **unknown beforehand**
- Use **principle of locality** to predict

---

## 📍 Principle of Locality

### 🕒 Temporal Locality

- If data is accessed now, it will likely be accessed again **soon**

### 🗺️ Spatial Locality

- If one address is accessed, nearby addresses will likely be accessed

---

## 📥 Cache Access Behavior

- **First access**:
  - Load from main memory to register
  - Store in cache
- **Subsequent accesses**:
  - Load from cache (faster)

---

## 🧮 Example: Sum Reduction

```c
float s[16] = 0;
for (i = 0; i < N; i++) {
  s[i%16] += a[i];
}
```

- `s`: good **temporal locality**
- `a`: good **spatial locality**

---

## 🧰 Cache Design Questions

1. Where to put loaded data?
2. How to check if data is in cache?
3. What to do when cache is full?

- Address broken into:
  - **Tag**
  - **Index**
  - **Offset**

---

## 📌 Direct Mapped Cache

- Each address maps to **one specific cache slot**
- Conflict resolution: **newer replaces older** (LRU)

---

## 📏 Cache Line Size

- Data is loaded one cache line at a time
- 64 bytes is typical
- Optimize algorithms to use **cache-line-sized chunks**

---

## 🔄 Cache Thrashing Example

```c
int a[64], b[64], r = 0;
for (int i = 0; i < 100; i++)
  for (int j = 0; j < 64; j++)
    r += a[j] + b[j];
```

- Although cache is big enough, **a[j] and b[j] map to the same cache line**, causing conflict.

---

## 🧩 Cache Associativity Types

| Type              | Description                             |
| ----------------- | --------------------------------------- |
| Direct Mapped     | One memory block maps to one cache line |
| Fully Associative | Can go anywhere, costly to implement    |
| Set Associative   | N-way set (common values: 2, 4, 8, 16)  |

---

## 📊 Exercise 2 & 3: Bandwidth vs Array Size

### Bandwidth from different levels:

- **L1** (<32KB): ~370GB/s
- **L2** (<512KB): ~100GB/s
- **L3** (<16MB): ~78GB/s
- **RAM**: ~17.5GB/s

> Performance decreases as data moves away from CPU.

---

## 🧠 Summary: Hardware Architecture Insights

### Key Factors:

- Instruction count
- Execution efficiency
- Data movement time

### Hardware Parallelism:

- **Sockets**: 1–4 CPUs
- **Cores**: 4–32 per CPU
- **Vectorization**: 2–16 floats per vector
- **Superscalar**: 2–8 instructions per cycle