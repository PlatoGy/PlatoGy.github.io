---
layout: post
title: GPUç¼–ç¨‹PPT2
author: Patrick Gao
date: 2025-04-08 03:17
categories:
  - MISCADA
  - GPU Course
tags:
  - GPU Programming
mermaid: true
math: true
pin: false
---
# CUDA Optimization Techniques / CUDA ä¼˜åŒ–æŠ€æœ¯  
**COMP52315 GPU Programming**  
Lecturer: Christopher Marcotte  
Email: christopher.marcotte@durham.ac.uk  

---

## ğŸ“š Recap / å¤ä¹ 

- **Concurrency vs. Parallelism / å¹¶å‘ vs. å¹¶è¡Œ**
- **Flynn's Taxonomy / Flynn åˆ†ç±»æ³•**
- **GPU Architecture / GPU æ¶æ„**:
  - Streaming Multiprocessors (SMs): FPUs, LD/ST, SFUs, Tensor Cores  
    æµå¤šå¤„ç†å™¨ï¼šæµ®ç‚¹å•å…ƒã€åŠ è½½/å­˜å‚¨å•å…ƒã€ç‰¹æ®Šå‡½æ•°å•å…ƒã€å¼ é‡æ ¸å¿ƒ
  - Cache / ç¼“å­˜
  - Warp Schedulers / Warpè°ƒåº¦å™¨
  - Global Memory / å…¨å±€å†…å­˜
- **SIMT Model / SIMT æ¨¡å‹**:
  - Kernel function executed by a grid of thread blocks  
    å†…æ ¸å‡½æ•°ç”±çº¿ç¨‹å—ç½‘æ ¼å¹¶è¡Œæ‰§è¡Œ
- **CPU vs GPU**:
  - CPU: latency-driven / å»¶è¿Ÿé©±åŠ¨
  - GPU: throughput-driven / ååé©±åŠ¨

---

## ğŸ”§ Performance Optimization Tooling / æ€§èƒ½ä¼˜åŒ–å·¥å…·

### Optimization Cycle / ä¼˜åŒ–å‘¨æœŸ

1. Write baseline code / ç¼–å†™åˆå§‹ä»£ç   
2. Tune kernel configuration / è°ƒæ•´å†…æ ¸é…ç½®  
3. Profile to find hotspots / ä½¿ç”¨åˆ†æå™¨è¯†åˆ«çƒ­ç‚¹  
4. Identify performance limiters / æ‰¾å‡ºæ€§èƒ½ç“¶é¢ˆ  
5. Optimize code / ä¼˜åŒ–ä»£ç 

### Nsight Compute (`ncu`)

- è®¾å¤‡ç«¯ CUDA åˆ†æå™¨  
- æ”¯æŒ GUI å’Œ CLI  
- ä½¿ç”¨ç¡¬ä»¶è®¡æ•°å™¨å’Œè½¯ä»¶æ’è£…æ¥è·å–æ€§èƒ½æŒ‡æ ‡  
- æä¾›åŸºäºè§„åˆ™çš„ä¼˜åŒ–å»ºè®®  
- æ³¨æ„ä¸è¦å’Œ Nsight Systems æ··æ·†ï¼ˆåè€…ç”¨äºç³»ç»Ÿçº§æ€§èƒ½åˆ†æï¼‰

---

## ğŸ¯ Occupancy / å ç”¨ç‡

- æ´»è·ƒ warp æ•°é‡ä¸ SM æœ€å¤§ warp æ•°çš„æ¯”ä¾‹  
- ä¼šéšæ—¶é—´å’Œ SM ä¸åŒè€Œå˜åŒ–  
- å ç”¨ç‡ä½ â†’ æŒ‡ä»¤å‘å°„æ•ˆç‡ä½ï¼Œéš¾ä»¥éšè—å»¶è¿Ÿ  
- é™åˆ¶å› ç´ ï¼š
  - æ¯çº¿ç¨‹ä½¿ç”¨çš„å¯„å­˜å™¨æ•°é‡  
  - æ¯çº¿ç¨‹å—çš„çº¿ç¨‹æ•°  
  - ä½¿ç”¨çš„å…±äº«å†…å­˜å¤§å°  
- ncu å¯é¢„æµ‹å¹¶æŠ¥å‘Šå ç”¨ç‡

---

## ğŸš¦ Application Performance Characteristics / ç¨‹åºæ€§èƒ½ç‰¹å¾

- **Memory-bound / å†…å­˜å—é™å‹**ï¼šå†…å­˜å¸¦å®½æ¥è¿‘ä¸Šé™  
- **Compute-bound / è®¡ç®—å—é™å‹**ï¼šæŒ‡ä»¤ååæ¥è¿‘ä¸Šé™  
- **Latency-bound / å»¶è¿Ÿå—é™å‹**ï¼šä¸»è¦ç“¶é¢ˆä¸ºå»¶è¿Ÿ/åŒæ­¥/ç­‰å¾…  
- æ€§èƒ½ç“¶é¢ˆä¾èµ–äºæ‰§è¡Œé˜¶æ®µå’Œä»£ç åŒºåŸŸ

---

## ğŸ“‹ ncu Profiling Commands / `ncu` åˆ†æå‘½ä»¤

```bash
nvcc -lineinfo saxpy.cu -o saxpy         # ç¼–è¯‘ï¼ŒåŒ…å«æºç è¡Œå·
ncu -o saxpy_prof_report saxpy           # ç”Ÿæˆé»˜è®¤åˆ†ææŠ¥å‘Š
ncu --set full -o full_prof_report saxpy # å…¨æŒ‡æ ‡åˆ†æ
ncu --section MemoryWorkloadAnalysis -o mem_prof_report saxpy # ç‰¹å®šåˆ†æ
```

Docs:  
- https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html  
- https://developer.nvidia.com/tools-downloads

---

## âš™ï¸ Kernel Configuration / å†…æ ¸é…ç½®

- å¯åŠ¨æ›´å¤šçº¿ç¨‹ä»¥éšè—å»¶è¿Ÿ  
- **BLOCK_SIZE**: çº¿ç¨‹å—å†…çº¿ç¨‹æ•°åº”ä¸º32çš„å€æ•°ï¼ˆwarpï¼‰  
- **GRID_SIZE**: çº¿ç¨‹å—æ•°é‡å»ºè®®ä¸º SM æ•°çš„å€æ•°  
- ä¸Šé™ä¼°ç®—ï¼š
  \[
  \left( rac{	ext{maxThreadsPerMultiprocessor}}{	ext{BLOCK\_SIZE}} 
ight) 	imes 	ext{multiProcessorCount}
  \]

---

## ğŸ”€ Branch Divergence / åˆ†æ”¯å‘æ•£

### åŸå› 

- Warp ä¸­çº¿ç¨‹éœ€è¦æ‰§è¡Œç›¸åŒæŒ‡ä»¤  
- æ¡ä»¶è¯­å¥ä½¿çº¿ç¨‹èµ°ä¸åŒè·¯å¾„ â†’ éƒ¨åˆ†çº¿ç¨‹è¢«å±è”½ç­‰å¾…  

### ç¤ºä¾‹

```cpp
// å‘æ•£ç‰ˆæœ¬
if (gid % 2 == 0) array[gid] = 0; 
else array[gid] = 1;

// éå‘æ•£ç‰ˆæœ¬
array[gid] = gid % 2;
```

### é¿å…æ–¹å¼

- æ‹†æˆå¤šä¸ªå†…æ ¸å‡½æ•°
- æ¡ä»¶åˆ†æ”¯å¯¹é½ warp è¾¹ç•Œ
- ä½¿ç”¨æ•°å­¦è¡¨è¾¾å¼ä»£æ›¿æ¡ä»¶è¯­å¥ï¼ˆå¦‚ sigmoidï¼‰

---

## ğŸ§  Memory Optimization / å†…å­˜ä¼˜åŒ–

### æ‰‹åŠ¨ vs ç®¡ç†å†…å­˜

| Host (CPU) | Bus (PCIe/NVLink) | Device (GPU) |
|------------|-------------------|---------------|
| `cudaMalloc/copy` | â¬…ï¸â¡ï¸ | `cudaMalloc` |
| `cudaMallocManaged` è‡ªåŠ¨ç®¡ç† | | |

---

## ğŸ§® Shared Memory / å…±äº«å†…å­˜

- ç›¸å½“äºæ‰‹åŠ¨ç®¡ç†çš„ç¼“å­˜ï¼ˆæ¯çº¿ç¨‹å—å¯é‡ç”¨ï¼‰  
- åˆ†ä¸ºå¤šä¸ª memory bankï¼Œåº”é¿å… bank å†²çªï¼ˆè¿ç»­è®¿é—®æœ€ä½³ï¼‰  

### ç¤ºä¾‹

```cpp
__shared__ float s[1024]; // é™æ€åˆ†é…
extern __shared__ float s[]; // åŠ¨æ€åˆ†é…ï¼Œè°ƒç”¨æ—¶æŒ‡å®šå¤§å°
```

---

## ğŸ§© Types of Parallelism / å¹¶è¡Œç±»å‹

1. **Data Parallelism / æ•°æ®å¹¶è¡Œ**ï¼šç›¸åŒæ“ä½œå¤„ç†å¤šä¸ªæ•°æ®  
2. **Task Parallelism / ä»»åŠ¡å¹¶è¡Œ**ï¼šåˆ†ä¸ºä¸åŒä»»åŠ¡å¹¶åˆ†æä¾èµ–  
3. **Pipelining / æµæ°´çº¿å¹¶è¡Œ**ï¼šæ•°æ®é€šè¿‡å¤šä¸ªé˜¶æ®µæµåŠ¨

---

## ğŸ”— Task Parallelism / ä»»åŠ¡å¹¶è¡Œ

- ä½¿ç”¨ä»»åŠ¡å›¾è¡¨è¾¾ï¼š  
  - èŠ‚ç‚¹è¡¨ç¤ºä»»åŠ¡ï¼Œè¾¹è¡¨ç¤ºä¾èµ–  
- æ”¯æŒä¸åŒç²’åº¦ï¼š
  - å‡½æ•°çº§ï¼šä»»åŠ¡ä¸ºå‡½æ•°è°ƒç”¨
  - æ“ä½œçº§ï¼šä»»åŠ¡ä¸ºè¿ç®—æ“ä½œ

---

## ğŸ” Dynamic Parallelism / åŠ¨æ€å¹¶è¡Œ

- å¯åœ¨è®¾å¤‡ç«¯å†…æ ¸ä¸­å†å‘èµ·å†…æ ¸  
- é€‚ç”¨äºé€’å½’é—®é¢˜  
- é¿å…è¿”å› CPU åŒæ­¥ï¼Œæé«˜æ•ˆç‡  

### ç¤ºä¾‹ï¼š

```cpp
__global__ void factorial(const int N, int *f){
  if (N >= 1) {
    *f *= N;
    factorial<<<1,1>>>(N-1, f);
  }
}
```

> ç¼–è¯‘æ—¶åŠ  `--rdc=true` ä½¿è®¾å¤‡ç«¯ä»£ç å¯é‡å®šä½

---

## ğŸŒ€ Stream Parallelism / æµå¹¶è¡Œ

- åŒä¸€ stream å†…æ“ä½œæŒ‰é¡ºåºæ‰§è¡Œ  
- ä¸åŒ stream å¯å¹¶è¡Œæ‰§è¡Œ  
- CUDA stream = ä¸€åˆ—æŒ‰é¡ºåºæ‰§è¡Œçš„ä»»åŠ¡

---

## â›“ï¸ Asynchronous Task Graphs / å¼‚æ­¥ä»»åŠ¡å›¾

- ä»»åŠ¡å›¾åŒ…å«ï¼š
  - CUDA å†…æ ¸
  - ä¸»æœºå‡½æ•°
  - å†…å­˜æ‹·è´/åˆ†é…
  - å­ä»»åŠ¡å›¾

### æ‰§è¡Œæµç¨‹ï¼š

1. åˆ›å»ºä»»åŠ¡å›¾  
2. å®ä¾‹åŒ–å›¾ä¸ºå¯æ‰§è¡Œå½¢å¼  
3. å¯åŠ¨ä»»åŠ¡å›¾

---

ğŸ“ æ–‡æ¡£é“¾æ¥ï¼š  
- https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#creating-a-graph-using-graph-apis  
- https://www.olcf.ornl.gov/wp-content/uploads/2021/10/013_CUDA_Graphs.pdf
